"Container orchestration tool"

Availability = no downtime
Scalability = load balancing
Recovery = backups and restoring


Master node and worker nodes
- Kubelet inside of worker nodes = lets cluster communicate
- Master node (control plane):
	- API Server = entry point to cluster (for API, UI, CLI)
	- Controller manager = keeps track of status of cluster
	- Scheduler = workload balancing across nodes
	- etcd = key/value storage, status data of whole cluster, snapshot (for backup/restoration)
	--> have multiple master node copies for safety
- Virtual network connects all of the individual nodes into one unified machine, allows communication

Pod = abstraction over container, so you only interact with k8s
- 1 app per pod
- gets its own IP for internal communication (between pods)
- ephemeral 

Service
- permanent IP attached to each pod so even when pod recreated the endpoint remains the same
- external/internal service based on needs
- communication
- helps with load balancing
- allows loose coupling while still communicating between pods
- selectors + targetPort define where and on what port to direct requests

- ClusterIP
	- default type
	- only accessible from within cluster, no outside access (only via ingress)
- Headless Service
	- direct communication with specific pods
	- often used with stateful services
	- if clusterIP is set to None get Pod IP instead
- NodePort
	- external access on static fixed port on each worker node
	- directly to node instead of through ingress
	- chooses a port to expose for this (30000-32767)
	- also needs to know clusterip service is accessible at
	- not efficient or secure
- LoadBalancer
	- accessible externally through cloud provider's load balancer
	- nodeport and clusterip created automatically

Ingress
- Forwarding to services
- routs traffic into cluster
- directs from external host to internal service
- takes advantage of entrypoint inside of cluster

- ingress controller (another pod) evaluates and processes ingress rules --> entrypoint
	- 3rd party implementation of controller


ConfigMap
- external configuration to application
- URL mapping (when changing names)
- so you don't have to go through the whole rebuild process with changes

Secrets
- like configmap but stored encoded/encrypted (w/ 3rd party tools)


Volume
- persistent db/log data when restarting db pod
- creates local storage of data
- not part of the cluster, but plugged into cluster (k8s cluster explicitly does not manage data persistence)
- provisioned before creation of cluster

- persistent volume
	- cluster resource (like ram or cpu)
	- stores data
	- needs physical storage  --> must be created and managed externally
	- not namespaced

- persistent volume claim
	- node makes request through pvc
	- pvc finds volume that meets criteria
	- volume connects to physical storage
	- volume mounted into pod then into container inside of pod

	- inside of namespace of node
	- abstracts the persistence away from devs

- storage class
	- previsions volumes dynamically when claimed
	- allows automation


Deployment
- Replicate everything from a node on an identical node (number specified in deployment [blueprint])
- Service manages which to use
- You don't create pods, you create deployments --> abstraction on top of pods
- Can't have deployments for db (data inconsistencies)
- Avoids downtime
- stateLESS

Stateful set
- specifically for db
- synchronizes reads and writes to avoid inconsistencies
- stateful deployment basically 
--> annoying, so common to host db outside of cluster
- each pod has their own unique identity even after rescheduling so not interchangeable
	- only one node can write (master)
	- not all pods have access to the same physical storage despite the same data (worker nodes must keep updated w/ changes to master)
	- ordered linearly (2 must exist for 3 to be created, 3 must be deleted before 2 can be deleted)


Master node (Configuration)
API Server
- entry point to cluster / enables interaction
- configuration requests sent here
- requests yaml or json format
- declarative = say what our desire is and k8s tries to meet that desire  --> is == should
- deployment and service
- config file:
	- metadata
	- specification --> attributes 
	- status (automatically generated by k8s)
		- comparing desired and actual and responding accordingly, self healing
		- this data comes from etcd
	--> store w/ code


minikube for local testing (+kind or k3d)
- master and worker in one node
- docker preinstalled

kubectl to interact w/ cluster


namespace = virtual cluster inside of cluster
- system
	- don't modify
	- system processes
	- master processes
- public
	- public data
	- configmap with cluster information
- node lease
	- information on availability of each node
	- each node has their own
- default
	- where everything goes if you don't define more specific namespaces
--> can be created w/ config file

- organizing/grouping different resource types
- not suggested for smaller projects
- keeping different projects separated avoids problem of same named deployments overriding each other, reuse components (common resources) for multiple clusters, blue green deployment, control access, resource control
- can access services from other namespaces
- volume and node can't be isolated in a namespace (they live globally)


Helm
- package manager
- distributing collections of yaml config files 	--> helm charts
- can be used for templates (property/type of yaml files)
- can redeploy same thing in different environment
- Tiller stores versions of deployment configs
	--> has a lot of permissions so a security vulnerability

- chart.yaml = metadata
- values.yaml = values for template
- charts/ = dependencies
- templates/ = template files



EKS
- Manages master nodes for you --> "Managed Kubernetes Service"
- Only create worker nodes yourself
- Reduces overhead 
	* ECS is AWS specific which makes it hard to migrate clusters to another platform later, EKS does not have that problem. EKS also gives access to all the other k8s tooling and community. ECS does have the benefit of ease for smaller apps 